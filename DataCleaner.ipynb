{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "\n",
    "PATH = \"data/\"\n",
    "CLEAN_AND_LEMMATIZE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve articles from disk\n",
    "\n",
    "print(\"Retrieving articles from disk...\")\n",
    "dataset = pd.read_csv(PATH + \"dataset.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stop words and lemmatization word net\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean article contents\n",
    "\n",
    "punctuation = string.punctuation + \"“”‘’0123456789―—–…\\n\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_article(article):\n",
    "    # Lowercase the article\n",
    "    article = article.lower()\n",
    "        \n",
    "    # Remove punctuation\n",
    "#     article = article.translate(str.maketrans(' ', ' ', '—–'))\n",
    "#     article = article.translate(str.maketrans('', '', \"—–\"))\n",
    "    article = article.translate(str.maketrans(' ', ' ', punctuation))\n",
    "        \n",
    "    if CLEAN_AND_LEMMATIZE:\n",
    "        # Remove stop words\n",
    "        words = article.split()\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "            \n",
    "        # Apply lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        article = \" \".join(words)\n",
    "            \n",
    "    return article\n",
    "\n",
    "print(\"Cleaning articles...\")\n",
    "dataset_clean = dataset\n",
    "dataset_clean[\"Article_Text\"] = dataset_clean[\"Article_Text\"].apply(clean_article)\n",
    "print(dataset_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to disk\n",
    "\n",
    "print(\"Saving dataset to disk\")\n",
    "Path(PATH).mkdir(parents=True, exist_ok=True)\n",
    "dataset_clean.to_csv(PATH + \"dataset_clean.csv\", index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
