{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "\n",
    "PATH = \"data/\"\n",
    "DATA_PATH=\"crawler_data/data/\"\n",
    "DOMAINS = {\n",
    "    \"nytimes.com\": 0,\n",
    "    \"washingtonpost.com\": 0,\n",
    "    \"huffpost.com\": 0,\n",
    "    \"vox.com\": 0,\n",
    "    \"foxnews.com\": 1,\n",
    "    \"washingtontimes.com\": 1,\n",
    "    \"breitbart.com\": 1,\n",
    "    \"nypost.com\": 1\n",
    "}\n",
    "NUM_ARTICLES_PER_DOMAIN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(source_domain):\n",
    "    return DOMAINS[source_domain] if source_domain in DOMAINS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles = []\n",
    "\n",
    "def walk_through_dir(dir):\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        files = (file_path for file_path in files if file_path.endswith(\".json\"))\n",
    "        for file_path in files:\n",
    "            with open(root + \"/\" + file_path) as file:\n",
    "                data = json.load(file)\n",
    "                if data[\"maintext\"] and data[\"source_domain\"]:\n",
    "                    label = get_label(data[\"source_domain\"])\n",
    "                    if label is not None:\n",
    "                        articles.append({\"Article_Title\": data[\"title\"],\n",
    "                                         \"Article_Text\": data[\"maintext\"],\n",
    "                                         \"Publish_Date\": data[\"date_publish\"],\n",
    "                                         \"Source\": data[\"source_domain\"],\n",
    "                                         \"Language\": data[\"language\"],\n",
    "                                         \"Label\": label})\n",
    "                    else:\n",
    "                        print(\"Could not get label for source_domain:\", data[\"source_domain\"])\n",
    "                else:\n",
    "                    print(\"Missing maintext or source_domain\")\n",
    "        for dir in dirs:\n",
    "            walk_through_dir(dir)\n",
    "\n",
    "walk_through_dir(DATA_PATH)\n",
    "dataset = pd.DataFrame(articles)\n",
    "print(Counter(dataset.loc[dataset[\"Language\"] == \"en\"][\"Source\"]))\n",
    "print(Counter(dataset.loc[dataset[\"Language\"] == \"en\"][\"Label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = pd.DataFrame(columns = [\"Article_Title\", \"Article_Text\", \"Source\", \"Label\"])\n",
    "\n",
    "for source in dataset[\"Source\"].unique():\n",
    "    articles = dataset.loc[(dataset[\"Source\"] == source) & (dataset[\"Language\"] == \"en\")]\n",
    "    articles = articles.sample(frac=1)\n",
    "    articles = articles[:NUM_ARTICLES_PER_DOMAIN]\n",
    "    filtered_dataset = filtered_dataset.append(articles)\n",
    "    \n",
    "print(Counter(filtered_dataset[\"Source\"]))\n",
    "print(Counter(filtered_dataset[\"Label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to disk\n",
    "\n",
    "print(\"Saving dataset to disk\")\n",
    "Path(PATH).mkdir(parents=True, exist_ok=True)\n",
    "dataset.to_csv(PATH + \"dataset.csv\", index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in filtered_dataset[\"Source\"].unique():\n",
    "    numWords = filtered_dataset.loc[filtered_dataset[\"Source\"] == source][\"Article_Text\"].str.split().apply(len)\n",
    "    plt.bar(source, numWords.mean())\n",
    "    \n",
    "plt.title(\"Average word count per publication\")\n",
    "plt.xticks(filtered_dataset[\"Source\"].unique(), rotation=45)\n",
    "plt.ylabel(\"Word count\")\n",
    "plt.xlabel(\"Publication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelNames = [\"Left-Leaning\", \"Right-Leaning\"]\n",
    "colors = [\"red\", \"blue\"]\n",
    "\n",
    "for label in filtered_dataset[\"Label\"].unique():\n",
    "    numWords = filtered_dataset.loc[filtered_dataset[\"Label\"] == label][\"Article_Text\"].str.split().apply(len)\n",
    "    plt.bar(labelNames[label], numWords.mean(), color=colors[label])\n",
    "    \n",
    "plt.title(\"Average word count per political leaning\")\n",
    "plt.ylabel(\"Word count\")\n",
    "plt.xlabel(\"Political Leaning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Average word count per political leaning\")\n",
    "\n",
    "sourceNames = [\"Vox\", \"NYPost\", \"HuffPost\", \"WashTimes\", \"WaPo\", \"FoxNews\", \"NYTimes\", \"Breitbart\"]\n",
    "labelNames = [\"Left-Leaning\", \"Right-Leaning\"]\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "for source, sourceName in zip(filtered_dataset[\"Source\"].unique(), sourceNames):\n",
    "    numWords = filtered_dataset.loc[filtered_dataset[\"Source\"] == source][\"Article_Text\"].str.split().apply(len)\n",
    "    plt.bar(sourceName, numWords.mean())\n",
    "    \n",
    "plt.ylabel(\"Word count\")\n",
    "plt.xticks(sourceNames, rotation=75)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "colors = [\"red\", \"blue\"]\n",
    "\n",
    "for label in filtered_dataset[\"Label\"].unique():\n",
    "    numWords = filtered_dataset.loc[filtered_dataset[\"Label\"] == label][\"Article_Text\"].str.split().apply(len)\n",
    "    plt.bar(labelNames[label], numWords.mean(), color=colors[label])\n",
    "\n",
    "plt.xticks(labelNames, rotation=35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
