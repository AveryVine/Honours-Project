{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, BatchNormalization, Activation, Dropout, SpatialDropout1D\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "\n",
    "PATH = \"data/\"\n",
    "NUM_FEATURES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Gensim data\n",
    "\n",
    "# word2vec = api.load(\"glove-twitter-25\")\n",
    "word2vec = api.load(\"glove-twitter-50\")\n",
    "# word2vec = api.load(\"glove-twitter-100\")\n",
    "# word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve vocabulary from disk\n",
    "\n",
    "print(\"Retrieving article data from disk...\")\n",
    "dataset = pd.read_csv(PATH + \"dataset_clean.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GENSIM = True\n",
    "MAX_SEQ_LEN = 500\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "x_train = train.loc[:, \"Article_Text\"].values\n",
    "y_train = train.loc[:, \"Label\"].values\n",
    "x_test = test.loc[:, \"Article_Text\"].values\n",
    "y_test = test.loc[:, \"Label\"].values\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "print(\"Tokenizing data...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(np.concatenate((x_train, x_valid, x_test)))\n",
    "\n",
    "print(\"Converting to sequences...\")\n",
    "x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "x_valid_sequences = tokenizer.texts_to_sequences(x_valid)\n",
    "x_test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "print(\"Padding sequences...\")\n",
    "x_train_pad = pad_sequences(x_train_sequences, maxlen=MAX_SEQ_LEN)\n",
    "x_valid_pad = pad_sequences(x_valid_sequences, maxlen=MAX_SEQ_LEN)\n",
    "x_test_pad = pad_sequences(x_test_sequences, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "print(\"\\tTrain Sequence Shape:\", x_train_pad.shape)\n",
    "print(\"\\tValid Sequence Shape:\", x_valid_pad.shape)\n",
    "print(\"\\tTest Sequence Shape:\", x_test_pad.shape)\n",
    "\n",
    "if USE_GENSIM:\n",
    "    print(\"Retrieving word embeddings from Gensim...\")\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, NUM_FEATURES))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words or not word in word2vec.vocab:\n",
    "            continue\n",
    "        embedding_vector = word2vec[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "VALIDATION_STEPS = 10\n",
    "USE_CONVOLUTION = False\n",
    "USE_BIDIRECTIONAL = True\n",
    "\n",
    "# Batch and prefetch the datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_pad, y_train))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid_pad, y_valid))\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
    "valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = Sequential()\n",
    "\n",
    "if USE_GENSIM:\n",
    "    model.add(Embedding(num_words, NUM_FEATURES, input_length=MAX_SEQ_LEN, weights=[embedding_matrix], trainable=False))\n",
    "else:\n",
    "    model.add(Embedding(num_words, NUM_FEATURES, input_length=MAX_SEQ_LEN))\n",
    "\n",
    "if USE_CONVOLUTION:\n",
    "    model.add(Conv1D(128, 3, padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "if USE_BIDIRECTIONAL:\n",
    "    model.add(Bidirectional(LSTM(50, dropout=0.5, recurrent_dropout=0.5)))\n",
    "else:\n",
    "    model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))\n",
    "    \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary(), \"\\n\")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=valid_dataset, validation_steps=VALIDATION_STEPS)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Graph the model history\n",
    "\n",
    "print(\"Training Accuracy:\", str(round(history.history['accuracy'][-1] * 100, 2)) + \"%\")\n",
    "print(\"Validation Accuracy:\", str(round(history.history['val_accuracy'][-1] * 100, 2)) + \"%\")\n",
    "plt.plot(history.history['accuracy'], color='blue')\n",
    "plt.plot(history.history['val_accuracy'], color='green')\n",
    "plt.title('Training vs. Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training Loss:\", str(round(history.history['loss'][-1] * 100, 2)) + \"%\")\n",
    "print(\"Validation Loss:\", str(round(history.history['val_loss'][-1] * 100, 2)) + \"%\")\n",
    "plt.plot(history.history['loss'], color='blue')\n",
    "plt.plot(history.history['val_loss'], color='green')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "score, acc = model.evaluate(x_test_pad, y_test, batch_size=BATCH_SIZE)\n",
    "print('Test accuracy:', str(round(acc * 100, 2)) + \"%\")\n",
    "print('Test loss:', str(round(score * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
